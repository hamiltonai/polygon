{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7eac0dfe-8631-4624-a07b-46b5aedfe3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ .env file loaded successfully\n",
      "üß™ No event provided - Running in JUPYTER/LOCAL mode with test event\n",
      "Test event: {'environment': 'test', 'max_workers': 10, 'ticker_limit': 100, 'page_limit': 5}\n",
      "‚úÖ AWS clients initialized successfully\n",
      "üìÇ No existing local file found, creating new: stock_data_20250604.csv\n",
      "Starting essential stock data collection...\n",
      "Test mode: Processing 5 sample tickers\n",
      "Starting parallel processing with 10 workers...\n",
      "Making 2 API calls per ticker (company info + OHLCV data)\n",
      "Processing completed in 0.5 seconds\n",
      "Valid records: 5, Errors: 0, API calls: 10\n",
      "üìÅ Data saved as local file: stock_data_20250604.csv\n",
      "\n",
      "============================================================\n",
      "üéâ DATA COLLECTION COMPLETE!\n",
      "============================================================\n",
      "üìä Processed 5 tickers\n",
      "‚úÖ Successful: 5 (100.0%)\n",
      "‚ùå Errors: 0\n",
      "‚è±Ô∏è  Time: 0.5s\n",
      "üîå API calls: 10 (2 per ticker)\n",
      "üìÅ File: stock_data_20250604.csv\n",
      "üîÑ Mode: Created new daily file\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import requests\n",
    "import boto3\n",
    "from io import StringIO\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import pytz\n",
    "import concurrent.futures\n",
    "from threading import Lock\n",
    "import time\n",
    "\n",
    "# Add dotenv support for local development\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()  # This will load your .env file from the same directory\n",
    "    print(\"‚úÖ .env file loaded successfully\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  python-dotenv not installed - using system environment variables\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Could not load .env file: {e}\")\n",
    "\n",
    "# Thread-safe counters\n",
    "progress_lock = Lock()\n",
    "processed_count = 0\n",
    "api_call_count = 0\n",
    "\n",
    "# Test event for when no event is provided (Jupyter/local testing)\n",
    "TEST_EVENT = {\n",
    "    \"environment\": \"test\",\n",
    "    \"max_workers\": 10,\n",
    "    \"ticker_limit\": 100,\n",
    "    \"page_limit\": 5\n",
    "}\n",
    "\n",
    "def make_polygon_request(url, headers, symbol=None, max_retries=3):\n",
    "    \"\"\"Make a Polygon API request with retry logic\"\"\"\n",
    "    global api_call_count\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            with progress_lock:\n",
    "                api_call_count += 1\n",
    "                \n",
    "            response = requests.get(url, headers=headers, timeout=15)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if data.get('status') == 'OK':\n",
    "                    return data\n",
    "                else:\n",
    "                    print(f\"API returned non-OK status for {symbol}: {data.get('status')}\")\n",
    "                    return None\n",
    "            elif response.status_code == 429:  # Rate limited\n",
    "                wait_time = 2 ** attempt\n",
    "                print(f\"Rate limited on {symbol}, waiting {wait_time}s...\")\n",
    "                time.sleep(wait_time)\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"HTTP {response.status_code} for {symbol}: {response.text[:200]}\")\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                print(f\"Final attempt failed for {symbol}: {e}\")\n",
    "                return None\n",
    "            else:\n",
    "                time.sleep(1)\n",
    "                continue\n",
    "    \n",
    "    return None\n",
    "\n",
    "def get_all_us_stocks(api_key, page_limit=None, per_page_limit=1000):\n",
    "    \"\"\"Get all US stocks from Polygon tickers endpoint\"\"\"\n",
    "    headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
    "    all_tickers = []\n",
    "    next_url = f\"https://api.polygon.io/v3/reference/tickers?market=stocks&locale=us&active=true&limit={per_page_limit}\"\n",
    "    \n",
    "    page_count = 0\n",
    "    max_pages = page_limit or 100  # Default safety limit, but configurable\n",
    "    while next_url and page_count < max_pages:\n",
    "        page_count += 1\n",
    "        print(f\"Fetching page {page_count} of tickers...\")\n",
    "        \n",
    "        data = make_polygon_request(next_url, headers, \"tickers_page\")\n",
    "        if not data or 'results' not in data:\n",
    "            break\n",
    "            \n",
    "        tickers = data['results']\n",
    "        all_tickers.extend([ticker['ticker'] for ticker in tickers])\n",
    "        \n",
    "        # Get next page URL\n",
    "        next_url = data.get('next_url')\n",
    "        if next_url:\n",
    "            # Check if next_url is already a full URL or just a path\n",
    "            if next_url.startswith('https://'):\n",
    "                # It's already a full URL, just add the API key\n",
    "                if '?' in next_url:\n",
    "                    next_url = f\"{next_url}&apikey={api_key}\"\n",
    "                else:\n",
    "                    next_url = f\"{next_url}?apikey={api_key}\"\n",
    "            else:\n",
    "                # It's just a path, prepend the domain\n",
    "                if '?' in next_url:\n",
    "                    next_url = f\"https://api.polygon.io{next_url}&apikey={api_key}\"\n",
    "                else:\n",
    "                    next_url = f\"https://api.polygon.io{next_url}?apikey={api_key}\"\n",
    "        \n",
    "        print(f\"Page {page_count}: Found {len(tickers)} tickers, total: {len(all_tickers)}\")\n",
    "        \n",
    "        # Small delay between pages\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "    if page_count >= max_pages:\n",
    "        print(f\"Reached page limit of {max_pages}. Total tickers collected: {len(all_tickers)}\")\n",
    "    else:\n",
    "        print(f\"Completed all pages. Total US stocks found: {len(all_tickers)}\")\n",
    "    return all_tickers\n",
    "\n",
    "def get_essential_stock_data(symbol, api_key):\n",
    "    \"\"\"Get essential trading data with minimal API calls (2 calls per ticker)\"\"\"\n",
    "    headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
    "    stock_data = {'ticker': symbol}\n",
    "    \n",
    "    # Get current time for column naming\n",
    "    cst = pytz.timezone('America/Chicago')\n",
    "    now_cst = datetime.now(cst)\n",
    "    time_str = now_cst.strftime('%H:%M')\n",
    "    \n",
    "    # 1. Company Details (for name and market cap)\n",
    "    company_url = f\"https://api.polygon.io/v3/reference/tickers/{symbol}\"\n",
    "    company_data = make_polygon_request(company_url, headers, symbol)\n",
    "    if company_data and 'results' in company_data:\n",
    "        result = company_data['results']\n",
    "        stock_data.update({\n",
    "            'name': result.get('name', 'N/A'),\n",
    "            'market_cap_millions': result.get('market_cap', 0) / 1_000_000 if result.get('market_cap') else 0,\n",
    "            'primary_exchange': result.get('primary_exchange', 'N/A')\n",
    "        })\n",
    "    else:\n",
    "        stock_data.update({\n",
    "            'name': 'N/A',\n",
    "            'market_cap_millions': 0,\n",
    "            'primary_exchange': 'N/A'\n",
    "        })\n",
    "    \n",
    "    # 2. Previous Day Aggregates (OHLCV)\n",
    "    prev_day_url = f\"https://api.polygon.io/v2/aggs/ticker/{symbol}/prev\"\n",
    "    prev_data = make_polygon_request(prev_day_url, headers, symbol)\n",
    "    if prev_data and 'results' in prev_data and len(prev_data['results']) > 0:\n",
    "        result = prev_data['results'][0]\n",
    "        \n",
    "        # Extract OHLCV data\n",
    "        open_price = result.get('o')\n",
    "        high_price = result.get('h')\n",
    "        low_price = result.get('l')\n",
    "        close_price = result.get('c')  # This is previous close\n",
    "        volume = result.get('v')\n",
    "        \n",
    "        stock_data.update({\n",
    "            'open': f\"{open_price:.2f}\" if open_price is not None else 'N/A',\n",
    "            'high': f\"{high_price:.2f}\" if high_price is not None else 'N/A',\n",
    "            'low': f\"{low_price:.2f}\" if low_price is not None else 'N/A',\n",
    "            'previous_close': f\"{close_price:.2f}\" if close_price is not None else 'N/A',\n",
    "            'volume': f\"{int(volume)}\" if volume is not None else 'N/A'\n",
    "        })\n",
    "        \n",
    "        # Current price with timestamp (using previous close as proxy)\n",
    "        current_price_col = f'current_price_{time_str}'\n",
    "        stock_data[current_price_col] = stock_data['previous_close']\n",
    "        \n",
    "        # Calculate open_pct_chg (current price vs open price)\n",
    "        open_pct_chg_col = f'open_pct_chg_{time_str}'\n",
    "        if open_price is not None and close_price is not None and open_price != 0:\n",
    "            pct_change = ((close_price - open_price) / open_price) * 100\n",
    "            stock_data[open_pct_chg_col] = f\"{pct_change:+.2f}%\"\n",
    "        else:\n",
    "            stock_data[open_pct_chg_col] = 'N/A'\n",
    "        \n",
    "        # Store raw values for potential previous_pct_chg calculation\n",
    "        stock_data['_raw_current_price'] = close_price if close_price is not None else None\n",
    "        stock_data['_current_time'] = time_str\n",
    "        \n",
    "    else:\n",
    "        stock_data.update({\n",
    "            'open': 'N/A',\n",
    "            'high': 'N/A', \n",
    "            'low': 'N/A',\n",
    "            'previous_close': 'N/A',\n",
    "            'volume': 'N/A'\n",
    "        })\n",
    "        \n",
    "        # Still need the timestamped columns even if no data\n",
    "        current_price_col = f'current_price_{time_str}'\n",
    "        open_pct_chg_col = f'open_pct_chg_{time_str}'\n",
    "        stock_data[current_price_col] = 'N/A'\n",
    "        stock_data[open_pct_chg_col] = 'N/A'\n",
    "        stock_data['_raw_current_price'] = None\n",
    "        stock_data['_current_time'] = time_str\n",
    "    \n",
    "    # Format market cap\n",
    "    if stock_data['market_cap_millions'] > 0:\n",
    "        stock_data['market_cap_millions'] = f\"{stock_data['market_cap_millions']:.2f}\"\n",
    "    else:\n",
    "        stock_data['market_cap_millions'] = 'N/A'\n",
    "    \n",
    "    return stock_data\n",
    "\n",
    "def process_single_ticker(symbol, api_key):\n",
    "    \"\"\"Process a single ticker with minimal API calls\"\"\"\n",
    "    global processed_count\n",
    "    \n",
    "    try:\n",
    "        stock_data = get_essential_stock_data(symbol, api_key)\n",
    "        \n",
    "        # Update progress counter\n",
    "        with progress_lock:\n",
    "            processed_count += 1\n",
    "            if processed_count % 100 == 0:\n",
    "                print(f\"Processed {processed_count} tickers... (API calls: {api_call_count})\")\n",
    "        \n",
    "        return stock_data, None\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error processing {symbol}: {str(e)}\"\n",
    "        print(error_msg)\n",
    "        return None, error_msg\n",
    "\n",
    "def lambda_handler(event=None, context=None):\n",
    "    global processed_count, api_call_count\n",
    "    processed_count = 0\n",
    "    api_call_count = 0\n",
    "    \n",
    "    # Auto-detect mode: if no event provided, assume Jupyter/local mode\n",
    "    jupyter_mode = event is None\n",
    "    \n",
    "    if jupyter_mode:\n",
    "        event = TEST_EVENT\n",
    "        print(\"üß™ No event provided - Running in JUPYTER/LOCAL mode with test event\")\n",
    "        print(f\"Test event: {event}\")\n",
    "    else:\n",
    "        print(\"üöÄ Event provided - Running in LAMBDA mode\")\n",
    "    \n",
    "    # Initialize AWS clients (skip if in Jupyter mode and no AWS setup)\n",
    "    try:\n",
    "        s3 = boto3.client('s3')\n",
    "        sns = boto3.client('sns')\n",
    "        aws_available = True\n",
    "        if jupyter_mode:\n",
    "            print(\"‚úÖ AWS clients initialized successfully\")\n",
    "    except Exception as e:\n",
    "        if jupyter_mode:\n",
    "            print(f\"‚ö†Ô∏è  AWS clients not available (expected in Jupyter): {e}\")\n",
    "            s3 = None\n",
    "            sns = None\n",
    "            aws_available = False\n",
    "        else:\n",
    "            raise e\n",
    "    \n",
    "    # Get environment variables\n",
    "    bucket_name = os.getenv('BUCKET_NAME')\n",
    "    polygon_api_key = os.getenv('POLYGON_API_KEY')\n",
    "    sns_topic_arn = os.getenv('SNS_TOPIC_ARN')\n",
    "    \n",
    "    if not polygon_api_key:\n",
    "        raise ValueError(\"POLYGON_API_KEY environment variable is required\")\n",
    "    \n",
    "    # Configuration\n",
    "    is_test = event.get('environment') == 'test'\n",
    "    max_workers = event.get('max_workers', 20)\n",
    "    ticker_limit = event.get('ticker_limit', 5000 if is_test else None)\n",
    "    page_limit = event.get('page_limit', None)\n",
    "    \n",
    "    # Generate file paths\n",
    "    input_date = datetime.now().strftime('%Y%m%d')\n",
    "    timestamp = datetime.now().strftime('%H%M')\n",
    "    \n",
    "    if jupyter_mode:\n",
    "        # Jupyter mode: Use simple daily filename for appending\n",
    "        output_file_name = f'stock_data_{input_date}.csv'\n",
    "        output_file_key = output_file_name\n",
    "    else:\n",
    "        # Lambda mode: Use S3 path structure\n",
    "        output_file_key = f'stock_data/{input_date}/stock_data_{input_date}.csv'\n",
    "    \n",
    "    # Check if we're appending to existing data\n",
    "    existing_data = {}\n",
    "    \n",
    "    if jupyter_mode:\n",
    "        # Try to read existing local file\n",
    "        try:\n",
    "            with open(output_file_name, 'r', encoding='utf-8') as f:\n",
    "                existing_content = f.read()\n",
    "                existing_reader = csv.DictReader(StringIO(existing_content))\n",
    "                for row in existing_reader:\n",
    "                    existing_data[row['ticker']] = row\n",
    "            print(f\"üìÇ Found existing local file with {len(existing_data)} tickers\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"üìÇ No existing local file found, creating new: {output_file_name}\")\n",
    "            existing_data = {}\n",
    "        except Exception as e:\n",
    "            print(f\"üìÇ Error reading existing file: {e}\")\n",
    "            existing_data = {}\n",
    "    elif aws_available:\n",
    "        # Try to read existing S3 file\n",
    "        try:\n",
    "            existing_obj = s3.get_object(Bucket=bucket_name, Key=output_file_key)\n",
    "            existing_content = existing_obj['Body'].read().decode('utf-8')\n",
    "            existing_reader = csv.DictReader(StringIO(existing_content))\n",
    "            for row in existing_reader:\n",
    "                existing_data[row['ticker']] = row\n",
    "            print(f\"üìÇ Found existing S3 file with {len(existing_data)} tickers\")\n",
    "        except Exception as e:\n",
    "            print(f\"üìÇ No existing S3 file found, creating new: {e}\")\n",
    "            existing_data = {}\n",
    "    \n",
    "    try:\n",
    "        print(\"Starting essential stock data collection...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Get all US stock tickers\n",
    "        if is_test:\n",
    "            tickers = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA'][:ticker_limit] if ticker_limit else ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA']\n",
    "            print(f\"Test mode: Processing {len(tickers)} sample tickers\")\n",
    "        else:\n",
    "            print(\"Fetching all US stock tickers...\")\n",
    "            all_tickers = get_all_us_stocks(polygon_api_key, page_limit=page_limit)\n",
    "            tickers = all_tickers[:ticker_limit] if ticker_limit else all_tickers\n",
    "            print(f\"Production mode: Processing {len(tickers)} tickers\")\n",
    "        \n",
    "        # Process all tickers in parallel\n",
    "        valid_data = []\n",
    "        errors = []\n",
    "        \n",
    "        print(f\"Starting parallel processing with {max_workers} workers...\")\n",
    "        print(f\"Making 2 API calls per ticker (company info + OHLCV data)\")\n",
    "        \n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            # Submit all tasks\n",
    "            future_to_ticker = {\n",
    "                executor.submit(process_single_ticker, ticker, polygon_api_key): ticker\n",
    "                for ticker in tickers\n",
    "            }\n",
    "            \n",
    "            # Collect results as they complete\n",
    "            for future in concurrent.futures.as_completed(future_to_ticker):\n",
    "                ticker = future_to_ticker[future]\n",
    "                try:\n",
    "                    stock_data, error = future.result()\n",
    "                    if stock_data:\n",
    "                        # Calculate previous_pct_chg if we have existing data\n",
    "                        if ticker in existing_data:\n",
    "                            current_time = stock_data['_current_time']\n",
    "                            current_price = stock_data['_raw_current_price']\n",
    "                            \n",
    "                            # Find the most recent previous current_price column\n",
    "                            prev_price_col = None\n",
    "                            prev_price_value = None\n",
    "                            \n",
    "                            for col, value in existing_data[ticker].items():\n",
    "                                if col.startswith('current_price_') and col != f'current_price_{current_time}':\n",
    "                                    if value != 'N/A':\n",
    "                                        try:\n",
    "                                            prev_price_value = float(value)\n",
    "                                            prev_price_col = col\n",
    "                                        except:\n",
    "                                            continue\n",
    "                            \n",
    "                            # Calculate previous_pct_chg\n",
    "                            prev_pct_chg_col = f'previous_pct_chg_{current_time}'\n",
    "                            if prev_price_value is not None and current_price is not None and prev_price_value != 0:\n",
    "                                pct_change = ((current_price - prev_price_value) / prev_price_value) * 100\n",
    "                                stock_data[prev_pct_chg_col] = f\"{pct_change:+.2f}%\"\n",
    "                            else:\n",
    "                                stock_data[prev_pct_chg_col] = 'N/A'\n",
    "                            \n",
    "                            # Merge with existing data (preserve old columns, add new ones)\n",
    "                            merged_data = existing_data[ticker].copy()\n",
    "                            merged_data.update(stock_data)\n",
    "                            stock_data = merged_data\n",
    "                        else:\n",
    "                            # New ticker, add empty previous_pct_chg\n",
    "                            current_time = stock_data['_current_time']\n",
    "                            prev_pct_chg_col = f'previous_pct_chg_{current_time}'\n",
    "                            stock_data[prev_pct_chg_col] = 'N/A'\n",
    "                        \n",
    "                        # Remove internal helper fields\n",
    "                        stock_data.pop('_raw_current_price', None)\n",
    "                        stock_data.pop('_current_time', None)\n",
    "                        \n",
    "                        valid_data.append(stock_data)\n",
    "                    if error:\n",
    "                        errors.append(error)\n",
    "                except Exception as e:\n",
    "                    error_msg = f\"Exception processing {ticker}: {e}\"\n",
    "                    print(error_msg)\n",
    "                    errors.append(error_msg)\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        print(f\"Processing completed in {processing_time:.1f} seconds\")\n",
    "        print(f\"Valid records: {len(valid_data)}, Errors: {len(errors)}, API calls: {api_call_count}\")\n",
    "        \n",
    "        if not valid_data:\n",
    "            raise Exception(\"No valid data collected\")\n",
    "        \n",
    "        # Get the current time for the dynamic column names\n",
    "        cst = pytz.timezone('America/Chicago')\n",
    "        now_cst = datetime.now(cst)\n",
    "        time_str = now_cst.strftime('%H:%M')\n",
    "        current_price_col = f'current_price_{time_str}'\n",
    "        open_pct_chg_col = f'open_pct_chg_{time_str}'\n",
    "        prev_pct_chg_col = f'previous_pct_chg_{time_str}'\n",
    "        \n",
    "        # Collect all possible columns from the data\n",
    "        all_columns = set()\n",
    "        for row in valid_data:\n",
    "            all_columns.update(row.keys())\n",
    "        \n",
    "        # Define base column order\n",
    "        base_columns = [\n",
    "            'ticker',\n",
    "            'name', \n",
    "            'market_cap_millions',\n",
    "            'volume',\n",
    "            'previous_close',\n",
    "            'open',\n",
    "            'high',\n",
    "            'low'\n",
    "        ]\n",
    "        \n",
    "        # Separate timestamped columns and sort them chronologically\n",
    "        current_price_cols = sorted([col for col in all_columns if col.startswith('current_price_')])\n",
    "        open_pct_chg_cols = sorted([col for col in all_columns if col.startswith('open_pct_chg_')])\n",
    "        prev_pct_chg_cols = sorted([col for col in all_columns if col.startswith('previous_pct_chg_')])\n",
    "        \n",
    "        # Build final column order: base columns + timestamped columns in chronological order\n",
    "        fieldnames = base_columns.copy()\n",
    "        \n",
    "        # Add timestamped columns in sets, chronologically\n",
    "        all_timestamps = set()\n",
    "        for col in current_price_cols + open_pct_chg_cols + prev_pct_chg_cols:\n",
    "            timestamp = col.split('_')[-1]  # Extract HH:MM\n",
    "            all_timestamps.add(timestamp)\n",
    "        \n",
    "        # Sort timestamps chronologically\n",
    "        sorted_timestamps = sorted(all_timestamps)\n",
    "        \n",
    "        # Add columns for each timestamp in order: current_price, open_pct_chg, previous_pct_chg\n",
    "        for timestamp in sorted_timestamps:\n",
    "            for col_type in ['current_price', 'open_pct_chg', 'previous_pct_chg']:\n",
    "                col_name = f'{col_type}_{timestamp}'\n",
    "                if col_name in all_columns:\n",
    "                    fieldnames.append(col_name)\n",
    "        \n",
    "        # Create CSV output\n",
    "        output = StringIO()\n",
    "        writer = csv.DictWriter(output, fieldnames=fieldnames, extrasaction='ignore')\n",
    "        writer.writeheader()\n",
    "        \n",
    "        # Sort by market cap (descending) then by ticker\n",
    "        def sort_key(item):\n",
    "            try:\n",
    "                market_cap = float(item.get('market_cap_millions', 0)) if item.get('market_cap_millions') != 'N/A' else 0\n",
    "                return (-market_cap, item.get('ticker', ''))\n",
    "            except:\n",
    "                return (0, item.get('ticker', ''))\n",
    "        \n",
    "        valid_data.sort(key=sort_key)\n",
    "        \n",
    "        for row in valid_data:\n",
    "            writer.writerow(row)\n",
    "        \n",
    "        # Handle output based on mode\n",
    "        if jupyter_mode:\n",
    "            # Save locally in Jupyter mode with append functionality\n",
    "            with open(output_file_name, 'w', newline='', encoding='utf-8') as f:\n",
    "                f.write(output.getvalue())\n",
    "            print(f\"üìÅ Data {'appended to' if existing_data else 'saved as'} local file: {output_file_name}\")\n",
    "            file_location = output_file_name\n",
    "        elif aws_available:\n",
    "            # Upload to S3 in Lambda mode\n",
    "            print(f\"Uploading data to S3: {output_file_key}\")\n",
    "            s3.put_object(\n",
    "                Bucket=bucket_name,\n",
    "                Key=output_file_key,\n",
    "                Body=output.getvalue(),\n",
    "                ContentType='text/csv'\n",
    "            )\n",
    "            file_location = output_file_key\n",
    "        else:\n",
    "            # Fallback: save locally even in Lambda mode if AWS not available\n",
    "            fallback_filename = f'stock_data_{input_date}_{timestamp}.csv'\n",
    "            with open(fallback_filename, 'w', newline='', encoding='utf-8') as f:\n",
    "                f.write(output.getvalue())\n",
    "            print(f\"üìÅ AWS not available, data saved locally as: {fallback_filename}\")\n",
    "            file_location = fallback_filename\n",
    "        \n",
    "        # Generate presigned URL (only if AWS available)\n",
    "        presigned_url = None\n",
    "        if aws_available and s3:\n",
    "            try:\n",
    "                presigned_url = s3.generate_presigned_url(\n",
    "                    'get_object',\n",
    "                    Params={'Bucket': bucket_name, 'Key': output_file_key},\n",
    "                    ExpiresIn=604800  # 7 days\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Could not generate presigned URL: {e}\")\n",
    "        \n",
    "        # Calculate statistics\n",
    "        success_rate = (len(valid_data) / len(tickers)) * 100 if tickers else 0\n",
    "        avg_api_calls_per_ticker = api_call_count / len(tickers) if tickers else 0\n",
    "        \n",
    "        # Send notification (only if AWS available)\n",
    "        if aws_available and sns:\n",
    "            message = (\n",
    "                f'üìä ESSENTIAL STOCK DATA COLLECTION COMPLETE\\n\\n'\n",
    "                f'üéØ STATISTICS:\\n'\n",
    "                f'‚Ä¢ Environment: {\"TEST\" if is_test else \"PRODUCTION\"}\\n'\n",
    "                f'‚Ä¢ Total tickers processed: {len(tickers):,}\\n'\n",
    "                f'‚Ä¢ Successful records: {len(valid_data):,}\\n'\n",
    "                f'‚Ä¢ Failed records: {len(errors):,}\\n'\n",
    "                f'‚Ä¢ Success rate: {success_rate:.1f}%\\n'\n",
    "                f'‚Ä¢ Processing time: {processing_time:.1f} seconds\\n'\n",
    "                f'‚Ä¢ API calls made: {api_call_count:,} (2 per ticker)\\n'\n",
    "                f'‚Ä¢ Avg API calls per ticker: {avg_api_calls_per_ticker:.1f}\\n\\n'\n",
    "                f'üìà DATA INCLUDES:\\n'\n",
    "                f'‚Ä¢ Ticker symbol\\n'\n",
    "                f'‚Ä¢ Company name\\n'\n",
    "                f'‚Ä¢ Market cap (millions)\\n'\n",
    "                f'‚Ä¢ Primary exchange\\n'\n",
    "                f'‚Ä¢ OHLCV data (Open, High, Low, Close, Volume)\\n'\n",
    "                f'‚Ä¢ Current price\\n\\n'\n",
    "                f'üíæ Download link (expires in 7 days):\\n{presigned_url or \"N/A\"}'\n",
    "            )\n",
    "            \n",
    "            if errors and len(errors) <= 10:\n",
    "                message += f'\\n\\n‚ùå ERRORS:\\n' + '\\n'.join(errors[:10])\n",
    "            elif errors:\n",
    "                message += f'\\n\\n‚ùå First 10 errors:\\n' + '\\n'.join(errors[:10])\n",
    "            \n",
    "            try:\n",
    "                sns.publish(\n",
    "                    TopicArn=sns_topic_arn,\n",
    "                    Subject=f'üìä Essential Stock Data Complete - {len(valid_data):,} stocks',\n",
    "                    Message=message\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Could not send SNS notification: {e}\")\n",
    "        \n",
    "        # Print summary for Jupyter mode\n",
    "        if jupyter_mode:\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"üéâ DATA COLLECTION COMPLETE!\")\n",
    "            print(\"=\"*60)\n",
    "            print(f\"üìä Processed {len(tickers):,} tickers\")\n",
    "            print(f\"‚úÖ Successful: {len(valid_data):,} ({success_rate:.1f}%)\")\n",
    "            print(f\"‚ùå Errors: {len(errors):,}\")\n",
    "            print(f\"‚è±Ô∏è  Time: {processing_time:.1f}s\")\n",
    "            print(f\"üîå API calls: {api_call_count:,} (2 per ticker)\")\n",
    "            print(f\"üìÅ File: {file_location}\")\n",
    "            print(f\"üîÑ Mode: {'Appended to existing' if existing_data else 'Created new'} daily file\")\n",
    "            if errors:\n",
    "                print(f\"\\nüö® Sample errors: {errors[:3]}\")\n",
    "        \n",
    "        return {\n",
    "            'statusCode': 200,\n",
    "            'body': {\n",
    "                'message': 'Successfully collected essential stock data',\n",
    "                'environment': 'test' if is_test else 'production',\n",
    "                'mode': 'jupyter' if jupyter_mode else 'lambda',\n",
    "                'append_mode': len(existing_data) > 0,\n",
    "                'existing_tickers': len(existing_data),\n",
    "                'total_tickers': len(tickers),\n",
    "                'successful_records': len(valid_data),\n",
    "                'errors': len(errors),\n",
    "                'success_rate': f\"{success_rate:.1f}%\",\n",
    "                'processing_time_seconds': round(processing_time, 1),\n",
    "                'api_calls_made': api_call_count,\n",
    "                'output_file': file_location,\n",
    "                'download_url': presigned_url\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_message = f'Error collecting essential stock data: {str(e)}'\n",
    "        print(f\"Exception: {error_message}\")\n",
    "        \n",
    "        # Send error notification (only if AWS available)\n",
    "        if aws_available and sns:\n",
    "            try:\n",
    "                sns.publish(\n",
    "                    TopicArn=sns_topic_arn,\n",
    "                    Subject='‚ùå Essential Stock Data Collection Failed',\n",
    "                    Message=f'Error: {error_message}\\n\\nProcessed {processed_count} tickers before failure.\\nAPI calls made: {api_call_count}'\n",
    "                )\n",
    "            except Exception as sns_error:\n",
    "                print(f\"Could not send error notification: {sns_error}\")\n",
    "        \n",
    "        return {\n",
    "            'statusCode': 500,\n",
    "            'body': {'error': error_message, 'processed_count': processed_count, 'api_calls': api_call_count}\n",
    "        }\n",
    "result = lambda_handler()\n",
    "# For Lambda: gets called automatically with event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf1e023f-9999-4cbe-849f-bca8963a3c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 01:54:15,318 - INFO - Created raw data file: nasdaq_monitor_raw_20250604_0154.csv\n",
      "2025-06-04 01:54:15,319 - INFO - Fetching NASDAQ symbols...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ .env file loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 01:54:16,875 - INFO - Page 1: Found 4965 symbols, total: 4965\n",
      "2025-06-04 01:54:16,876 - INFO - Found 4920 NASDAQ symbols\n",
      "2025-06-04 01:54:16,876 - INFO - Fetching initial data for all symbols...\n",
      "2025-06-04 01:54:17,536 - INFO - Initial data fetched for 4866 symbols using snapshot API\n",
      "2025-06-04 01:54:17,541 - INFO - Subscribing to 4920 symbols...\n",
      "2025-06-04 01:54:17,542 - INFO - Subscribing to batch 1/50\n",
      "2025-06-04 01:54:17,643 - INFO - Subscribing to batch 2/50\n",
      "2025-06-04 01:54:17,745 - INFO - Subscribing to batch 3/50\n",
      "2025-06-04 01:54:17,847 - INFO - Subscribing to batch 4/50\n",
      "2025-06-04 01:54:17,949 - INFO - Subscribing to batch 5/50\n",
      "2025-06-04 01:54:18,051 - INFO - Subscribing to batch 6/50\n",
      "2025-06-04 01:54:18,153 - INFO - Subscribing to batch 7/50\n",
      "2025-06-04 01:54:18,255 - INFO - Subscribing to batch 8/50\n",
      "2025-06-04 01:54:18,357 - INFO - Subscribing to batch 9/50\n",
      "2025-06-04 01:54:18,459 - INFO - Subscribing to batch 10/50\n",
      "2025-06-04 01:54:18,561 - INFO - Subscribing to batch 11/50\n",
      "2025-06-04 01:54:18,663 - INFO - Subscribing to batch 12/50\n",
      "2025-06-04 01:54:18,766 - INFO - Subscribing to batch 13/50\n",
      "2025-06-04 01:54:18,868 - INFO - Subscribing to batch 14/50\n",
      "2025-06-04 01:54:18,970 - INFO - Subscribing to batch 15/50\n",
      "2025-06-04 01:54:19,072 - INFO - Subscribing to batch 16/50\n",
      "2025-06-04 01:54:19,174 - INFO - Subscribing to batch 17/50\n",
      "2025-06-04 01:54:19,276 - INFO - Subscribing to batch 18/50\n",
      "2025-06-04 01:54:19,378 - INFO - Subscribing to batch 19/50\n",
      "2025-06-04 01:54:19,481 - INFO - Subscribing to batch 20/50\n",
      "2025-06-04 01:54:19,582 - INFO - Subscribing to batch 21/50\n",
      "2025-06-04 01:54:19,684 - INFO - Subscribing to batch 22/50\n",
      "2025-06-04 01:54:19,786 - INFO - Subscribing to batch 23/50\n",
      "2025-06-04 01:54:19,887 - INFO - Subscribing to batch 24/50\n",
      "2025-06-04 01:54:19,990 - INFO - Subscribing to batch 25/50\n",
      "2025-06-04 01:54:20,092 - INFO - Subscribing to batch 26/50\n",
      "2025-06-04 01:54:20,194 - INFO - Subscribing to batch 27/50\n",
      "2025-06-04 01:54:20,295 - INFO - Subscribing to batch 28/50\n",
      "2025-06-04 01:54:20,397 - INFO - Subscribing to batch 29/50\n",
      "2025-06-04 01:54:20,499 - INFO - Subscribing to batch 30/50\n",
      "2025-06-04 01:54:20,601 - INFO - Subscribing to batch 31/50\n",
      "2025-06-04 01:54:20,703 - INFO - Subscribing to batch 32/50\n",
      "2025-06-04 01:54:20,805 - INFO - Subscribing to batch 33/50\n",
      "2025-06-04 01:54:20,907 - INFO - Subscribing to batch 34/50\n",
      "2025-06-04 01:54:21,009 - INFO - Subscribing to batch 35/50\n",
      "2025-06-04 01:54:21,111 - INFO - Subscribing to batch 36/50\n",
      "2025-06-04 01:54:21,212 - INFO - Subscribing to batch 37/50\n",
      "2025-06-04 01:54:21,314 - INFO - Subscribing to batch 38/50\n",
      "2025-06-04 01:54:21,416 - INFO - Subscribing to batch 39/50\n",
      "2025-06-04 01:54:21,517 - INFO - Subscribing to batch 40/50\n",
      "2025-06-04 01:54:21,619 - INFO - Subscribing to batch 41/50\n",
      "2025-06-04 01:54:21,721 - INFO - Subscribing to batch 42/50\n",
      "2025-06-04 01:54:21,823 - INFO - Subscribing to batch 43/50\n",
      "2025-06-04 01:54:21,925 - INFO - Subscribing to batch 44/50\n",
      "2025-06-04 01:54:22,027 - INFO - Subscribing to batch 45/50\n",
      "2025-06-04 01:54:22,128 - INFO - Subscribing to batch 46/50\n",
      "2025-06-04 01:54:22,231 - INFO - Subscribing to batch 47/50\n",
      "2025-06-04 01:54:22,332 - INFO - Subscribing to batch 48/50\n",
      "2025-06-04 01:54:22,435 - INFO - Subscribing to batch 49/50\n",
      "2025-06-04 01:54:22,537 - INFO - Subscribing to batch 50/50\n",
      "2025-06-04 01:54:22,639 - INFO - Subscriptions complete. Starting real-time monitoring...\n",
      "2025-06-04 01:54:22,639 - ERROR - Error in WebSocket run loop: asyncio.run() cannot be called from a running event loop\n",
      "/var/folders/34/q7q3ndyd25zcv5f9h1sxjjgr0000gn/T/ipykernel_59678/632390914.py:410: RuntimeWarning: coroutine 'WebSocketClient.connect' was never awaited\n",
      "  logger.error(f\"Error in WebSocket run loop: {e}\")\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/var/folders/34/q7q3ndyd25zcv5f9h1sxjjgr0000gn/T/ipykernel_59678/632390914.py:413: RuntimeWarning: coroutine 'WebSocketClient.close' was never awaited\n",
      "  self.ws_client.close()\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "2025-06-04 01:54:22,643 - INFO - Monitor stopped\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "from collections import defaultdict\n",
    "import boto3\n",
    "from io import StringIO\n",
    "import logging\n",
    "from typing import Dict, List, Set\n",
    "import time\n",
    "\n",
    "# Polygon WebSocket imports\n",
    "from polygon import WebSocketClient\n",
    "from polygon.websocket.models import WebSocketMessage, EquityTrade, EquityQuote, EquityAgg\n",
    "from polygon import RESTClient\n",
    "\n",
    "# Load environment variables\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    print(\"‚úÖ .env file loaded successfully\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  python-dotenv not installed - using system environment variables\")\n",
    "\n",
    "# Configuration\n",
    "AWS_S3_ENABLED = True  # Toggle S3 upload\n",
    "S3_BUCKET = os.getenv('BUCKET_NAME')\n",
    "S3_PREFIX = \"stock_data/real-time-monitor/\"\n",
    "POLL_INTERVAL = 60  # seconds\n",
    "FILTER_START_DELAY = 420  # 7 minutes (420 seconds)\n",
    "POLYGON_API_KEY = os.getenv('POLYGON_API_KEY')\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class NASDAQMonitor:\n",
    "    def __init__(self):\n",
    "        self.rest_client = RESTClient(POLYGON_API_KEY)\n",
    "        self.ws_client = WebSocketClient(POLYGON_API_KEY, feed='sip')  # Premium real-time feed\n",
    "        self.stocks_data = defaultdict(dict)  # Store latest data for each stock\n",
    "        self.nasdaq_symbols = set()\n",
    "        self.qualified_symbols = set()\n",
    "        self.start_time = None\n",
    "        self.filter_enabled = False\n",
    "        self.running = True\n",
    "        self.cst = pytz.timezone('America/Chicago')\n",
    "        self.data_lock = asyncio.Lock()\n",
    "        \n",
    "        # S3 client\n",
    "        if AWS_S3_ENABLED:\n",
    "            self.s3_client = boto3.client('s3')\n",
    "        \n",
    "        # File paths\n",
    "        self.date_str = datetime.now(self.cst).strftime('%Y%m%d')\n",
    "        self.start_time_str = datetime.now(self.cst).strftime('%H%M')\n",
    "        self.raw_file = f'nasdaq_monitor_raw_{self.date_str}_{self.start_time_str}.csv'\n",
    "        self.filtered_file = f'nasdaq_monitor_filtered_{self.date_str}_{self.start_time_str}.csv'\n",
    "        \n",
    "        # CSV headers\n",
    "        self.headers = [\n",
    "            'timestamp', 'symbol', 'market_cap_millions', 'previous_close', 'open',\n",
    "            'current_price', 'bid', 'ask', 'volume', 'day_high', 'day_low',\n",
    "            'change_pct', 'change_from_open_pct', 'meets_criteria'\n",
    "        ]\n",
    "        \n",
    "        # Initialize CSV files\n",
    "        self._initialize_csv_files()\n",
    "    \n",
    "    def _initialize_csv_files(self):\n",
    "        \"\"\"Create CSV files with headers\"\"\"\n",
    "        with open(self.raw_file, 'w', newline='') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=self.headers)\n",
    "            writer.writeheader()\n",
    "        logger.info(f\"Created raw data file: {self.raw_file}\")\n",
    "    \n",
    "    async def fetch_nasdaq_symbols(self):\n",
    "        \"\"\"Fetch all NASDAQ symbols from Polygon using efficient pagination\"\"\"\n",
    "        logger.info(\"Fetching NASDAQ symbols...\")\n",
    "        symbols = []\n",
    "        \n",
    "        try:\n",
    "            # Use pagination efficiently\n",
    "            next_url = None\n",
    "            page_count = 0\n",
    "            \n",
    "            while True:\n",
    "                page_count += 1\n",
    "                if next_url:\n",
    "                    # Parse the cursor from next_url\n",
    "                    tickers_response = self.rest_client.list_tickers(\n",
    "                        market=\"stocks\",\n",
    "                        exchange=\"XNAS\",\n",
    "                        active=True,\n",
    "                        limit=1000,\n",
    "                        cursor=next_url.split('cursor=')[1] if 'cursor=' in next_url else None\n",
    "                    )\n",
    "                else:\n",
    "                    tickers_response = self.rest_client.list_tickers(\n",
    "                        market=\"stocks\",\n",
    "                        exchange=\"XNAS\",\n",
    "                        active=True,\n",
    "                        limit=1000\n",
    "                    )\n",
    "                \n",
    "                # Process the current page\n",
    "                page_symbols = []\n",
    "                for ticker in tickers_response:\n",
    "                    page_symbols.append(ticker.ticker)\n",
    "                \n",
    "                symbols.extend(page_symbols)\n",
    "                logger.info(f\"Page {page_count}: Found {len(page_symbols)} symbols, total: {len(symbols)}\")\n",
    "                \n",
    "                # Check if there's a next page\n",
    "                if hasattr(tickers_response, 'next_url') and tickers_response.next_url:\n",
    "                    next_url = tickers_response.next_url\n",
    "                else:\n",
    "                    break\n",
    "                \n",
    "                # Safety limit\n",
    "                if len(symbols) >= 10000:\n",
    "                    logger.warning(f\"Reached safety limit of 10000 symbols\")\n",
    "                    break\n",
    "            \n",
    "            self.nasdaq_symbols = set(symbols)\n",
    "            logger.info(f\"Found {len(self.nasdaq_symbols)} NASDAQ symbols\")\n",
    "            \n",
    "            # Fetch initial data using efficient snapshot API\n",
    "            await self.fetch_initial_data()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error fetching NASDAQ symbols: {e}\")\n",
    "            # Fallback to a small test set\n",
    "            self.nasdaq_symbols = {'AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA', 'META', 'NVDA', 'AMD', 'NFLX', 'TSLA'}\n",
    "            logger.info(f\"Using test set of {len(self.nasdaq_symbols)} symbols\")\n",
    "    \n",
    "    async def fetch_initial_data(self):\n",
    "        \"\"\"Fetch initial data (market cap, open, previous close) for all symbols using minimal API calls\"\"\"\n",
    "        logger.info(\"Fetching initial data for all symbols...\")\n",
    "        \n",
    "        # Use snapshot endpoint for bulk data - much more efficient\n",
    "        try:\n",
    "            # Get all snapshots in one call\n",
    "            snapshots = self.rest_client.get_snapshot_all(\"stocks\")\n",
    "            \n",
    "            for snapshot in snapshots:\n",
    "                symbol = snapshot.ticker\n",
    "                if symbol in self.nasdaq_symbols:\n",
    "                    async with self.data_lock:\n",
    "                        self.stocks_data[symbol].update({\n",
    "                            'market_cap_millions': getattr(snapshot, 'market_cap', 0) / 1_000_000 if hasattr(snapshot, 'market_cap') else 0,\n",
    "                            'previous_close': snapshot.prev_day.close if snapshot.prev_day else 0,\n",
    "                            'open': snapshot.day.open if snapshot.day else 0,\n",
    "                            'volume': snapshot.day.volume if snapshot.day else 0,\n",
    "                            'day_high': snapshot.day.high if snapshot.day else 0,\n",
    "                            'day_low': snapshot.day.low if snapshot.day else float('inf'),\n",
    "                            'current_price': snapshot.day.close if snapshot.day else 0\n",
    "                        })\n",
    "            \n",
    "            logger.info(f\"Initial data fetched for {len(self.stocks_data)} symbols using snapshot API\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error fetching snapshot data: {e}\")\n",
    "            # Fallback to individual calls only if snapshot fails\n",
    "            logger.info(\"Falling back to individual API calls...\")\n",
    "            \n",
    "            # Limit to top symbols to minimize calls\n",
    "            limited_symbols = list(self.nasdaq_symbols)[:100]\n",
    "            for symbol in limited_symbols:\n",
    "                try:\n",
    "                    # Get previous day data\n",
    "                    prev_day = self.rest_client.get_previous_close(symbol)\n",
    "                    if prev_day and len(prev_day) > 0:\n",
    "                        async with self.data_lock:\n",
    "                            self.stocks_data[symbol].update({\n",
    "                                'market_cap_millions': 0,  # Skip market cap in fallback\n",
    "                                'previous_close': prev_day[0].close,\n",
    "                                'open': prev_day[0].open,\n",
    "                                'volume': 0,\n",
    "                                'day_high': 0,\n",
    "                                'day_low': float('inf')\n",
    "                            })\n",
    "                except Exception as e:\n",
    "                    logger.debug(f\"Error fetching data for {symbol}: {e}\")\n",
    "    \n",
    "    def calculate_qualifying_criteria(self, symbol: str) -> bool:\n",
    "        \"\"\"Check if stock meets all qualifying criteria\"\"\"\n",
    "        data = self.stocks_data.get(symbol, {})\n",
    "        \n",
    "        # Get required values\n",
    "        volume = data.get('volume', 0)\n",
    "        prev_close = data.get('previous_close', 0)\n",
    "        open_price = data.get('open', 0)\n",
    "        current_price = data.get('current_price', 0)\n",
    "        \n",
    "        # Calculate change from previous close\n",
    "        if prev_close > 0:\n",
    "            change_pct = ((current_price - prev_close) / prev_close) * 100\n",
    "        else:\n",
    "            change_pct = 0\n",
    "        \n",
    "        # Check all criteria\n",
    "        meets_criteria = (\n",
    "            volume > 300_000 and\n",
    "            change_pct >= 2.5 and\n",
    "            prev_close >= 0.01 and\n",
    "            current_price > open_price\n",
    "        )\n",
    "        \n",
    "        return meets_criteria\n",
    "    \n",
    "    async def handle_message(self, message: WebSocketMessage):\n",
    "        \"\"\"Handle incoming WebSocket messages\"\"\"\n",
    "        if isinstance(message, (EquityTrade, EquityQuote, EquityAgg)):\n",
    "            symbol = message.symbol\n",
    "            \n",
    "            async with self.data_lock:\n",
    "                if symbol not in self.stocks_data:\n",
    "                    self.stocks_data[symbol] = {}\n",
    "                \n",
    "                # Update price and volume data\n",
    "                if isinstance(message, EquityTrade):\n",
    "                    self.stocks_data[symbol]['current_price'] = message.price\n",
    "                    self.stocks_data[symbol]['volume'] = self.stocks_data[symbol].get('volume', 0) + message.size\n",
    "                    \n",
    "                    # Update high/low\n",
    "                    current_high = self.stocks_data[symbol].get('day_high', 0)\n",
    "                    current_low = self.stocks_data[symbol].get('day_low', float('inf'))\n",
    "                    self.stocks_data[symbol]['day_high'] = max(current_high, message.price)\n",
    "                    self.stocks_data[symbol]['day_low'] = min(current_low, message.price)\n",
    "                \n",
    "                elif isinstance(message, EquityQuote):\n",
    "                    self.stocks_data[symbol]['bid'] = message.bid_price\n",
    "                    self.stocks_data[symbol]['ask'] = message.ask_price\n",
    "                \n",
    "                elif isinstance(message, EquityAgg):\n",
    "                    self.stocks_data[symbol]['current_price'] = message.close\n",
    "                    self.stocks_data[symbol]['volume'] = message.volume\n",
    "                    self.stocks_data[symbol]['day_high'] = message.high\n",
    "                    self.stocks_data[symbol]['day_low'] = message.low\n",
    "    \n",
    "    async def write_data_snapshot(self):\n",
    "        \"\"\"Write current data snapshot to CSV\"\"\"\n",
    "        timestamp = datetime.now(self.cst).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        rows = []\n",
    "        \n",
    "        async with self.data_lock:\n",
    "            for symbol in self.nasdaq_symbols:\n",
    "                if symbol not in self.stocks_data:\n",
    "                    continue\n",
    "                \n",
    "                data = self.stocks_data[symbol]\n",
    "                \n",
    "                # Skip if no current price\n",
    "                if 'current_price' not in data:\n",
    "                    continue\n",
    "                \n",
    "                # Calculate percentages\n",
    "                prev_close = data.get('previous_close', 0)\n",
    "                open_price = data.get('open', 0)\n",
    "                current_price = data.get('current_price', 0)\n",
    "                \n",
    "                change_pct = ((current_price - prev_close) / prev_close * 100) if prev_close > 0 else 0\n",
    "                change_from_open_pct = ((current_price - open_price) / open_price * 100) if open_price > 0 else 0\n",
    "                \n",
    "                # Check if meets criteria\n",
    "                meets_criteria = self.calculate_qualifying_criteria(symbol)\n",
    "                \n",
    "                row = {\n",
    "                    'timestamp': timestamp,\n",
    "                    'symbol': symbol,\n",
    "                    'market_cap_millions': f\"{data.get('market_cap_millions', 0):.2f}\",\n",
    "                    'previous_close': f\"{prev_close:.2f}\",\n",
    "                    'open': f\"{open_price:.2f}\",\n",
    "                    'current_price': f\"{current_price:.2f}\",\n",
    "                    'bid': f\"{data.get('bid', 0):.2f}\",\n",
    "                    'ask': f\"{data.get('ask', 0):.2f}\",\n",
    "                    'volume': data.get('volume', 0),\n",
    "                    'day_high': f\"{data.get('day_high', 0):.2f}\",\n",
    "                    'day_low': f\"{data.get('day_low', 0):.2f}\" if data.get('day_low', float('inf')) != float('inf') else \"0.00\",\n",
    "                    'change_pct': f\"{change_pct:.2f}\",\n",
    "                    'change_from_open_pct': f\"{change_from_open_pct:.2f}\",\n",
    "                    'meets_criteria': 'Y' if meets_criteria else 'N'\n",
    "                }\n",
    "                rows.append(row)\n",
    "                \n",
    "                # Track qualified symbols\n",
    "                if meets_criteria:\n",
    "                    self.qualified_symbols.add(symbol)\n",
    "                elif symbol in self.qualified_symbols:\n",
    "                    self.qualified_symbols.remove(symbol)\n",
    "        \n",
    "        # Write to raw file\n",
    "        with open(self.raw_file, 'a', newline='') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=self.headers)\n",
    "            writer.writerows(rows)\n",
    "        \n",
    "        # Write to filtered file if enabled and there are qualified stocks\n",
    "        if self.filter_enabled and self.qualified_symbols:\n",
    "            filtered_rows = [row for row in rows if row['meets_criteria'] == 'Y']\n",
    "            \n",
    "            # Create filtered file if it doesn't exist\n",
    "            if not os.path.exists(self.filtered_file):\n",
    "                with open(self.filtered_file, 'w', newline='') as f:\n",
    "                    writer = csv.DictWriter(f, fieldnames=self.headers)\n",
    "                    writer.writeheader()\n",
    "            \n",
    "            with open(self.filtered_file, 'a', newline='') as f:\n",
    "                writer = csv.DictWriter(f, fieldnames=self.headers)\n",
    "                writer.writerows(filtered_rows)\n",
    "        \n",
    "        # Upload to S3 if enabled\n",
    "        if AWS_S3_ENABLED:\n",
    "            await self.upload_to_s3()\n",
    "        \n",
    "        logger.info(f\"Data snapshot written - Total: {len(rows)}, Qualified: {len(self.qualified_symbols)}\")\n",
    "        \n",
    "        # Log when no qualifiers but keep running\n",
    "        if self.filter_enabled and len(self.qualified_symbols) == 0:\n",
    "            logger.info(\"No qualifying stocks at this time, continuing to monitor...\")\n",
    "    \n",
    "    async def upload_to_s3(self):\n",
    "        \"\"\"Upload CSV files to S3\"\"\"\n",
    "        try:\n",
    "            # Upload raw file\n",
    "            with open(self.raw_file, 'rb') as f:\n",
    "                self.s3_client.put_object(\n",
    "                    Bucket=S3_BUCKET,\n",
    "                    Key=f\"{S3_PREFIX}{self.raw_file}\",\n",
    "                    Body=f.read()\n",
    "                )\n",
    "            \n",
    "            # Upload filtered file if it exists\n",
    "            if os.path.exists(self.filtered_file):\n",
    "                with open(self.filtered_file, 'rb') as f:\n",
    "                    self.s3_client.put_object(\n",
    "                        Bucket=S3_BUCKET,\n",
    "                        Key=f\"{S3_PREFIX}{self.filtered_file}\",\n",
    "                        Body=f.read()\n",
    "                    )\n",
    "            \n",
    "            logger.info(\"Files uploaded to S3\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error uploading to S3: {e}\")\n",
    "    \n",
    "    async def periodic_writer(self):\n",
    "        \"\"\"Write data snapshots every minute\"\"\"\n",
    "        while self.running:\n",
    "            await asyncio.sleep(POLL_INTERVAL)\n",
    "            \n",
    "            # Enable filtering after 7 minutes\n",
    "            if not self.filter_enabled and self.start_time:\n",
    "                elapsed = time.time() - self.start_time\n",
    "                if elapsed >= FILTER_START_DELAY:\n",
    "                    self.filter_enabled = True\n",
    "                    logger.info(\"Filtering enabled - creating filtered output file\")\n",
    "            \n",
    "            await self.write_data_snapshot()\n",
    "    \n",
    "    async def run(self):\n",
    "        \"\"\"Main run loop\"\"\"\n",
    "        self.start_time = time.time()\n",
    "        \n",
    "        # Fetch NASDAQ symbols\n",
    "        await self.fetch_nasdaq_symbols()\n",
    "        \n",
    "        # Subscribe to all symbols\n",
    "        logger.info(f\"Subscribing to {len(self.nasdaq_symbols)} symbols...\")\n",
    "        \n",
    "        # Subscribe in batches to avoid overwhelming the connection\n",
    "        symbols_list = list(self.nasdaq_symbols)\n",
    "        batch_size = 100\n",
    "        \n",
    "        # Create subscription strings for efficiency\n",
    "        subscription_strings = []\n",
    "        for i in range(0, len(symbols_list), batch_size):\n",
    "            batch = symbols_list[i:i + batch_size]\n",
    "            # Create comma-separated symbol list for each batch\n",
    "            batch_str = \",\".join(batch)\n",
    "            subscription_strings.append(batch_str)\n",
    "        \n",
    "        # Subscribe to all data types for each batch\n",
    "        for i, batch_str in enumerate(subscription_strings):\n",
    "            logger.info(f\"Subscribing to batch {i+1}/{len(subscription_strings)}\")\n",
    "            \n",
    "            # Subscribe to trades\n",
    "            self.ws_client.subscribe(f\"T.{batch_str}\")\n",
    "            # Subscribe to quotes  \n",
    "            self.ws_client.subscribe(f\"Q.{batch_str}\")\n",
    "            # Subscribe to minute aggregates\n",
    "            self.ws_client.subscribe(f\"AM.{batch_str}\")\n",
    "            \n",
    "            await asyncio.sleep(0.1)  # Small delay between batches\n",
    "        \n",
    "        logger.info(\"Subscriptions complete. Starting real-time monitoring...\")\n",
    "        \n",
    "        # Start periodic writer\n",
    "        writer_task = asyncio.create_task(self.periodic_writer())\n",
    "        \n",
    "        # Define sync handler to bridge to async\n",
    "        def sync_handle_message(message):\n",
    "            asyncio.create_task(self.handle_message(message))\n",
    "\n",
    "        try:\n",
    "            self.ws_client.run(sync_handle_message)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in WebSocket run loop: {e}\")\n",
    "        finally:\n",
    "            writer_task.cancel()\n",
    "            await self.ws_client.close()\n",
    "            logger.info(\"Monitor stopped\")\n",
    "\n",
    "async def main():\n",
    "    \"\"\"Main function to run the monitor\"\"\"\n",
    "    monitor = NASDAQMonitor()\n",
    "    \n",
    "    try:\n",
    "        await monitor.run()\n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(\"Received keyboard interrupt, shutting down...\")\n",
    "        monitor.running = False\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in main loop: {e}\")\n",
    "        monitor.running = False\n",
    "\n",
    "# Run the monitor\n",
    "if __name__ == \"__main__\":\n",
    "    # For Jupyter notebook, use:\n",
    "    await main()\n",
    "    \n",
    "    # For script execution, use:\n",
    "    # asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6507c2-aebd-4058-909a-5d034e92a23f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
