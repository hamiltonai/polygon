{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7eac0dfe-8631-4624-a07b-46b5aedfe3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ .env file loaded successfully\n",
      "üß™ No event provided - Running in JUPYTER/LOCAL mode with test event\n",
      "Test event: {'environment': 'test', 'max_workers': 10, 'ticker_limit': 100, 'page_limit': 5}\n",
      "‚úÖ AWS clients initialized successfully\n",
      "üìÇ Found existing local file with 5 tickers\n",
      "Starting essential stock data collection...\n",
      "Test mode: Processing 5 sample tickers\n",
      "Starting parallel processing with 10 workers...\n",
      "Making 2 API calls per ticker (company info + OHLCV data)\n",
      "Processing completed in 0.4 seconds\n",
      "Valid records: 5, Errors: 0, API calls: 10\n",
      "üìÅ Data appended to local file: stock_data_20250601.csv\n",
      "\n",
      "============================================================\n",
      "üéâ DATA COLLECTION COMPLETE!\n",
      "============================================================\n",
      "üìä Processed 5 tickers\n",
      "‚úÖ Successful: 5 (100.0%)\n",
      "‚ùå Errors: 0\n",
      "‚è±Ô∏è  Time: 0.4s\n",
      "üîå API calls: 10 (2 per ticker)\n",
      "üìÅ File: stock_data_20250601.csv\n",
      "üîÑ Mode: Appended to existing daily file\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import requests\n",
    "import boto3\n",
    "from io import StringIO\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import pytz\n",
    "import concurrent.futures\n",
    "from threading import Lock\n",
    "import time\n",
    "\n",
    "# Add dotenv support for local development\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()  # This will load your .env file from the same directory\n",
    "    print(\"‚úÖ .env file loaded successfully\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  python-dotenv not installed - using system environment variables\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Could not load .env file: {e}\")\n",
    "\n",
    "# Thread-safe counters\n",
    "progress_lock = Lock()\n",
    "processed_count = 0\n",
    "api_call_count = 0\n",
    "\n",
    "# Test event for when no event is provided (Jupyter/local testing)\n",
    "TEST_EVENT = {\n",
    "    \"environment\": \"test\",\n",
    "    \"max_workers\": 10,\n",
    "    \"ticker_limit\": 100,\n",
    "    \"page_limit\": 5\n",
    "}\n",
    "\n",
    "def make_polygon_request(url, headers, symbol=None, max_retries=3):\n",
    "    \"\"\"Make a Polygon API request with retry logic\"\"\"\n",
    "    global api_call_count\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            with progress_lock:\n",
    "                api_call_count += 1\n",
    "                \n",
    "            response = requests.get(url, headers=headers, timeout=15)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if data.get('status') == 'OK':\n",
    "                    return data\n",
    "                else:\n",
    "                    print(f\"API returned non-OK status for {symbol}: {data.get('status')}\")\n",
    "                    return None\n",
    "            elif response.status_code == 429:  # Rate limited\n",
    "                wait_time = 2 ** attempt\n",
    "                print(f\"Rate limited on {symbol}, waiting {wait_time}s...\")\n",
    "                time.sleep(wait_time)\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"HTTP {response.status_code} for {symbol}: {response.text[:200]}\")\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                print(f\"Final attempt failed for {symbol}: {e}\")\n",
    "                return None\n",
    "            else:\n",
    "                time.sleep(1)\n",
    "                continue\n",
    "    \n",
    "    return None\n",
    "\n",
    "def get_all_us_stocks(api_key, page_limit=None, per_page_limit=1000):\n",
    "    \"\"\"Get all US stocks from Polygon tickers endpoint\"\"\"\n",
    "    headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
    "    all_tickers = []\n",
    "    next_url = f\"https://api.polygon.io/v3/reference/tickers?market=stocks&locale=us&active=true&limit={per_page_limit}\"\n",
    "    \n",
    "    page_count = 0\n",
    "    max_pages = page_limit or 100  # Default safety limit, but configurable\n",
    "    while next_url and page_count < max_pages:\n",
    "        page_count += 1\n",
    "        print(f\"Fetching page {page_count} of tickers...\")\n",
    "        \n",
    "        data = make_polygon_request(next_url, headers, \"tickers_page\")\n",
    "        if not data or 'results' not in data:\n",
    "            break\n",
    "            \n",
    "        tickers = data['results']\n",
    "        all_tickers.extend([ticker['ticker'] for ticker in tickers])\n",
    "        \n",
    "        # Get next page URL\n",
    "        next_url = data.get('next_url')\n",
    "        if next_url:\n",
    "            # Check if next_url is already a full URL or just a path\n",
    "            if next_url.startswith('https://'):\n",
    "                # It's already a full URL, just add the API key\n",
    "                if '?' in next_url:\n",
    "                    next_url = f\"{next_url}&apikey={api_key}\"\n",
    "                else:\n",
    "                    next_url = f\"{next_url}?apikey={api_key}\"\n",
    "            else:\n",
    "                # It's just a path, prepend the domain\n",
    "                if '?' in next_url:\n",
    "                    next_url = f\"https://api.polygon.io{next_url}&apikey={api_key}\"\n",
    "                else:\n",
    "                    next_url = f\"https://api.polygon.io{next_url}?apikey={api_key}\"\n",
    "        \n",
    "        print(f\"Page {page_count}: Found {len(tickers)} tickers, total: {len(all_tickers)}\")\n",
    "        \n",
    "        # Small delay between pages\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "    if page_count >= max_pages:\n",
    "        print(f\"Reached page limit of {max_pages}. Total tickers collected: {len(all_tickers)}\")\n",
    "    else:\n",
    "        print(f\"Completed all pages. Total US stocks found: {len(all_tickers)}\")\n",
    "    return all_tickers\n",
    "\n",
    "def get_essential_stock_data(symbol, api_key):\n",
    "    \"\"\"Get essential trading data with minimal API calls (2 calls per ticker)\"\"\"\n",
    "    headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
    "    stock_data = {'ticker': symbol}\n",
    "    \n",
    "    # Get current time for column naming\n",
    "    cst = pytz.timezone('America/Chicago')\n",
    "    now_cst = datetime.now(cst)\n",
    "    time_str = now_cst.strftime('%H:%M')\n",
    "    \n",
    "    # 1. Company Details (for name and market cap)\n",
    "    company_url = f\"https://api.polygon.io/v3/reference/tickers/{symbol}\"\n",
    "    company_data = make_polygon_request(company_url, headers, symbol)\n",
    "    if company_data and 'results' in company_data:\n",
    "        result = company_data['results']\n",
    "        stock_data.update({\n",
    "            'name': result.get('name', 'N/A'),\n",
    "            'market_cap_millions': result.get('market_cap', 0) / 1_000_000 if result.get('market_cap') else 0,\n",
    "            'primary_exchange': result.get('primary_exchange', 'N/A')\n",
    "        })\n",
    "    else:\n",
    "        stock_data.update({\n",
    "            'name': 'N/A',\n",
    "            'market_cap_millions': 0,\n",
    "            'primary_exchange': 'N/A'\n",
    "        })\n",
    "    \n",
    "    # 2. Previous Day Aggregates (OHLCV)\n",
    "    prev_day_url = f\"https://api.polygon.io/v2/aggs/ticker/{symbol}/prev\"\n",
    "    prev_data = make_polygon_request(prev_day_url, headers, symbol)\n",
    "    if prev_data and 'results' in prev_data and len(prev_data['results']) > 0:\n",
    "        result = prev_data['results'][0]\n",
    "        \n",
    "        # Extract OHLCV data\n",
    "        open_price = result.get('o')\n",
    "        high_price = result.get('h')\n",
    "        low_price = result.get('l')\n",
    "        close_price = result.get('c')  # This is previous close\n",
    "        volume = result.get('v')\n",
    "        \n",
    "        stock_data.update({\n",
    "            'open': f\"{open_price:.2f}\" if open_price is not None else 'N/A',\n",
    "            'high': f\"{high_price:.2f}\" if high_price is not None else 'N/A',\n",
    "            'low': f\"{low_price:.2f}\" if low_price is not None else 'N/A',\n",
    "            'previous_close': f\"{close_price:.2f}\" if close_price is not None else 'N/A',\n",
    "            'volume': f\"{int(volume)}\" if volume is not None else 'N/A'\n",
    "        })\n",
    "        \n",
    "        # Current price with timestamp (using previous close as proxy)\n",
    "        current_price_col = f'current_price_{time_str}'\n",
    "        stock_data[current_price_col] = stock_data['previous_close']\n",
    "        \n",
    "        # Calculate open_pct_chg (current price vs open price)\n",
    "        open_pct_chg_col = f'open_pct_chg_{time_str}'\n",
    "        if open_price is not None and close_price is not None and open_price != 0:\n",
    "            pct_change = ((close_price - open_price) / open_price) * 100\n",
    "            stock_data[open_pct_chg_col] = f\"{pct_change:+.2f}%\"\n",
    "        else:\n",
    "            stock_data[open_pct_chg_col] = 'N/A'\n",
    "        \n",
    "        # Store raw values for potential previous_pct_chg calculation\n",
    "        stock_data['_raw_current_price'] = close_price if close_price is not None else None\n",
    "        stock_data['_current_time'] = time_str\n",
    "        \n",
    "    else:\n",
    "        stock_data.update({\n",
    "            'open': 'N/A',\n",
    "            'high': 'N/A', \n",
    "            'low': 'N/A',\n",
    "            'previous_close': 'N/A',\n",
    "            'volume': 'N/A'\n",
    "        })\n",
    "        \n",
    "        # Still need the timestamped columns even if no data\n",
    "        current_price_col = f'current_price_{time_str}'\n",
    "        open_pct_chg_col = f'open_pct_chg_{time_str}'\n",
    "        stock_data[current_price_col] = 'N/A'\n",
    "        stock_data[open_pct_chg_col] = 'N/A'\n",
    "        stock_data['_raw_current_price'] = None\n",
    "        stock_data['_current_time'] = time_str\n",
    "    \n",
    "    # Format market cap\n",
    "    if stock_data['market_cap_millions'] > 0:\n",
    "        stock_data['market_cap_millions'] = f\"{stock_data['market_cap_millions']:.2f}\"\n",
    "    else:\n",
    "        stock_data['market_cap_millions'] = 'N/A'\n",
    "    \n",
    "    return stock_data\n",
    "\n",
    "def process_single_ticker(symbol, api_key):\n",
    "    \"\"\"Process a single ticker with minimal API calls\"\"\"\n",
    "    global processed_count\n",
    "    \n",
    "    try:\n",
    "        stock_data = get_essential_stock_data(symbol, api_key)\n",
    "        \n",
    "        # Update progress counter\n",
    "        with progress_lock:\n",
    "            processed_count += 1\n",
    "            if processed_count % 100 == 0:\n",
    "                print(f\"Processed {processed_count} tickers... (API calls: {api_call_count})\")\n",
    "        \n",
    "        return stock_data, None\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error processing {symbol}: {str(e)}\"\n",
    "        print(error_msg)\n",
    "        return None, error_msg\n",
    "\n",
    "def lambda_handler(event=None, context=None):\n",
    "    global processed_count, api_call_count\n",
    "    processed_count = 0\n",
    "    api_call_count = 0\n",
    "    \n",
    "    # Auto-detect mode: if no event provided, assume Jupyter/local mode\n",
    "    jupyter_mode = event is None\n",
    "    \n",
    "    if jupyter_mode:\n",
    "        event = TEST_EVENT\n",
    "        print(\"üß™ No event provided - Running in JUPYTER/LOCAL mode with test event\")\n",
    "        print(f\"Test event: {event}\")\n",
    "    else:\n",
    "        print(\"üöÄ Event provided - Running in LAMBDA mode\")\n",
    "    \n",
    "    # Initialize AWS clients (skip if in Jupyter mode and no AWS setup)\n",
    "    try:\n",
    "        s3 = boto3.client('s3')\n",
    "        sns = boto3.client('sns')\n",
    "        aws_available = True\n",
    "        if jupyter_mode:\n",
    "            print(\"‚úÖ AWS clients initialized successfully\")\n",
    "    except Exception as e:\n",
    "        if jupyter_mode:\n",
    "            print(f\"‚ö†Ô∏è  AWS clients not available (expected in Jupyter): {e}\")\n",
    "            s3 = None\n",
    "            sns = None\n",
    "            aws_available = False\n",
    "        else:\n",
    "            raise e\n",
    "    \n",
    "    # Get environment variables\n",
    "    bucket_name = os.getenv('BUCKET_NAME')\n",
    "    polygon_api_key = os.getenv('POLYGON_API_KEY')\n",
    "    sns_topic_arn = os.getenv('SNS_TOPIC_ARN')\n",
    "    \n",
    "    if not polygon_api_key:\n",
    "        raise ValueError(\"POLYGON_API_KEY environment variable is required\")\n",
    "    \n",
    "    # Configuration\n",
    "    is_test = event.get('environment') == 'test'\n",
    "    max_workers = event.get('max_workers', 20)\n",
    "    ticker_limit = event.get('ticker_limit', 5000 if is_test else None)\n",
    "    page_limit = event.get('page_limit', None)\n",
    "    \n",
    "    # Generate file paths\n",
    "    input_date = datetime.now().strftime('%Y%m%d')\n",
    "    timestamp = datetime.now().strftime('%H%M')\n",
    "    \n",
    "    if jupyter_mode:\n",
    "        # Jupyter mode: Use simple daily filename for appending\n",
    "        output_file_name = f'stock_data_{input_date}.csv'\n",
    "        output_file_key = output_file_name\n",
    "    else:\n",
    "        # Lambda mode: Use S3 path structure\n",
    "        output_file_key = f'stock_data/{input_date}/stock_data_{input_date}.csv'\n",
    "    \n",
    "    # Check if we're appending to existing data\n",
    "    existing_data = {}\n",
    "    \n",
    "    if jupyter_mode:\n",
    "        # Try to read existing local file\n",
    "        try:\n",
    "            with open(output_file_name, 'r', encoding='utf-8') as f:\n",
    "                existing_content = f.read()\n",
    "                existing_reader = csv.DictReader(StringIO(existing_content))\n",
    "                for row in existing_reader:\n",
    "                    existing_data[row['ticker']] = row\n",
    "            print(f\"üìÇ Found existing local file with {len(existing_data)} tickers\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"üìÇ No existing local file found, creating new: {output_file_name}\")\n",
    "            existing_data = {}\n",
    "        except Exception as e:\n",
    "            print(f\"üìÇ Error reading existing file: {e}\")\n",
    "            existing_data = {}\n",
    "    elif aws_available:\n",
    "        # Try to read existing S3 file\n",
    "        try:\n",
    "            existing_obj = s3.get_object(Bucket=bucket_name, Key=output_file_key)\n",
    "            existing_content = existing_obj['Body'].read().decode('utf-8')\n",
    "            existing_reader = csv.DictReader(StringIO(existing_content))\n",
    "            for row in existing_reader:\n",
    "                existing_data[row['ticker']] = row\n",
    "            print(f\"üìÇ Found existing S3 file with {len(existing_data)} tickers\")\n",
    "        except Exception as e:\n",
    "            print(f\"üìÇ No existing S3 file found, creating new: {e}\")\n",
    "            existing_data = {}\n",
    "    \n",
    "    try:\n",
    "        print(\"Starting essential stock data collection...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Get all US stock tickers\n",
    "        if is_test:\n",
    "            tickers = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA'][:ticker_limit] if ticker_limit else ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA']\n",
    "            print(f\"Test mode: Processing {len(tickers)} sample tickers\")\n",
    "        else:\n",
    "            print(\"Fetching all US stock tickers...\")\n",
    "            all_tickers = get_all_us_stocks(polygon_api_key, page_limit=page_limit)\n",
    "            tickers = all_tickers[:ticker_limit] if ticker_limit else all_tickers\n",
    "            print(f\"Production mode: Processing {len(tickers)} tickers\")\n",
    "        \n",
    "        # Process all tickers in parallel\n",
    "        valid_data = []\n",
    "        errors = []\n",
    "        \n",
    "        print(f\"Starting parallel processing with {max_workers} workers...\")\n",
    "        print(f\"Making 2 API calls per ticker (company info + OHLCV data)\")\n",
    "        \n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            # Submit all tasks\n",
    "            future_to_ticker = {\n",
    "                executor.submit(process_single_ticker, ticker, polygon_api_key): ticker\n",
    "                for ticker in tickers\n",
    "            }\n",
    "            \n",
    "            # Collect results as they complete\n",
    "            for future in concurrent.futures.as_completed(future_to_ticker):\n",
    "                ticker = future_to_ticker[future]\n",
    "                try:\n",
    "                    stock_data, error = future.result()\n",
    "                    if stock_data:\n",
    "                        # Calculate previous_pct_chg if we have existing data\n",
    "                        if ticker in existing_data:\n",
    "                            current_time = stock_data['_current_time']\n",
    "                            current_price = stock_data['_raw_current_price']\n",
    "                            \n",
    "                            # Find the most recent previous current_price column\n",
    "                            prev_price_col = None\n",
    "                            prev_price_value = None\n",
    "                            \n",
    "                            for col, value in existing_data[ticker].items():\n",
    "                                if col.startswith('current_price_') and col != f'current_price_{current_time}':\n",
    "                                    if value != 'N/A':\n",
    "                                        try:\n",
    "                                            prev_price_value = float(value)\n",
    "                                            prev_price_col = col\n",
    "                                        except:\n",
    "                                            continue\n",
    "                            \n",
    "                            # Calculate previous_pct_chg\n",
    "                            prev_pct_chg_col = f'previous_pct_chg_{current_time}'\n",
    "                            if prev_price_value is not None and current_price is not None and prev_price_value != 0:\n",
    "                                pct_change = ((current_price - prev_price_value) / prev_price_value) * 100\n",
    "                                stock_data[prev_pct_chg_col] = f\"{pct_change:+.2f}%\"\n",
    "                            else:\n",
    "                                stock_data[prev_pct_chg_col] = 'N/A'\n",
    "                            \n",
    "                            # Merge with existing data (preserve old columns, add new ones)\n",
    "                            merged_data = existing_data[ticker].copy()\n",
    "                            merged_data.update(stock_data)\n",
    "                            stock_data = merged_data\n",
    "                        else:\n",
    "                            # New ticker, add empty previous_pct_chg\n",
    "                            current_time = stock_data['_current_time']\n",
    "                            prev_pct_chg_col = f'previous_pct_chg_{current_time}'\n",
    "                            stock_data[prev_pct_chg_col] = 'N/A'\n",
    "                        \n",
    "                        # Remove internal helper fields\n",
    "                        stock_data.pop('_raw_current_price', None)\n",
    "                        stock_data.pop('_current_time', None)\n",
    "                        \n",
    "                        valid_data.append(stock_data)\n",
    "                    if error:\n",
    "                        errors.append(error)\n",
    "                except Exception as e:\n",
    "                    error_msg = f\"Exception processing {ticker}: {e}\"\n",
    "                    print(error_msg)\n",
    "                    errors.append(error_msg)\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        print(f\"Processing completed in {processing_time:.1f} seconds\")\n",
    "        print(f\"Valid records: {len(valid_data)}, Errors: {len(errors)}, API calls: {api_call_count}\")\n",
    "        \n",
    "        if not valid_data:\n",
    "            raise Exception(\"No valid data collected\")\n",
    "        \n",
    "        # Get the current time for the dynamic column names\n",
    "        cst = pytz.timezone('America/Chicago')\n",
    "        now_cst = datetime.now(cst)\n",
    "        time_str = now_cst.strftime('%H:%M')\n",
    "        current_price_col = f'current_price_{time_str}'\n",
    "        open_pct_chg_col = f'open_pct_chg_{time_str}'\n",
    "        prev_pct_chg_col = f'previous_pct_chg_{time_str}'\n",
    "        \n",
    "        # Collect all possible columns from the data\n",
    "        all_columns = set()\n",
    "        for row in valid_data:\n",
    "            all_columns.update(row.keys())\n",
    "        \n",
    "        # Define base column order\n",
    "        base_columns = [\n",
    "            'ticker',\n",
    "            'name', \n",
    "            'market_cap_millions',\n",
    "            'volume',\n",
    "            'previous_close',\n",
    "            'open',\n",
    "            'high',\n",
    "            'low'\n",
    "        ]\n",
    "        \n",
    "        # Separate timestamped columns and sort them chronologically\n",
    "        current_price_cols = sorted([col for col in all_columns if col.startswith('current_price_')])\n",
    "        open_pct_chg_cols = sorted([col for col in all_columns if col.startswith('open_pct_chg_')])\n",
    "        prev_pct_chg_cols = sorted([col for col in all_columns if col.startswith('previous_pct_chg_')])\n",
    "        \n",
    "        # Build final column order: base columns + timestamped columns in chronological order\n",
    "        fieldnames = base_columns.copy()\n",
    "        \n",
    "        # Add timestamped columns in sets, chronologically\n",
    "        all_timestamps = set()\n",
    "        for col in current_price_cols + open_pct_chg_cols + prev_pct_chg_cols:\n",
    "            timestamp = col.split('_')[-1]  # Extract HH:MM\n",
    "            all_timestamps.add(timestamp)\n",
    "        \n",
    "        # Sort timestamps chronologically\n",
    "        sorted_timestamps = sorted(all_timestamps)\n",
    "        \n",
    "        # Add columns for each timestamp in order: current_price, open_pct_chg, previous_pct_chg\n",
    "        for timestamp in sorted_timestamps:\n",
    "            for col_type in ['current_price', 'open_pct_chg', 'previous_pct_chg']:\n",
    "                col_name = f'{col_type}_{timestamp}'\n",
    "                if col_name in all_columns:\n",
    "                    fieldnames.append(col_name)\n",
    "        \n",
    "        # Create CSV output\n",
    "        output = StringIO()\n",
    "        writer = csv.DictWriter(output, fieldnames=fieldnames, extrasaction='ignore')\n",
    "        writer.writeheader()\n",
    "        \n",
    "        # Sort by market cap (descending) then by ticker\n",
    "        def sort_key(item):\n",
    "            try:\n",
    "                market_cap = float(item.get('market_cap_millions', 0)) if item.get('market_cap_millions') != 'N/A' else 0\n",
    "                return (-market_cap, item.get('ticker', ''))\n",
    "            except:\n",
    "                return (0, item.get('ticker', ''))\n",
    "        \n",
    "        valid_data.sort(key=sort_key)\n",
    "        \n",
    "        for row in valid_data:\n",
    "            writer.writerow(row)\n",
    "        \n",
    "        # Handle output based on mode\n",
    "        if jupyter_mode:\n",
    "            # Save locally in Jupyter mode with append functionality\n",
    "            with open(output_file_name, 'w', newline='', encoding='utf-8') as f:\n",
    "                f.write(output.getvalue())\n",
    "            print(f\"üìÅ Data {'appended to' if existing_data else 'saved as'} local file: {output_file_name}\")\n",
    "            file_location = output_file_name\n",
    "        elif aws_available:\n",
    "            # Upload to S3 in Lambda mode\n",
    "            print(f\"Uploading data to S3: {output_file_key}\")\n",
    "            s3.put_object(\n",
    "                Bucket=bucket_name,\n",
    "                Key=output_file_key,\n",
    "                Body=output.getvalue(),\n",
    "                ContentType='text/csv'\n",
    "            )\n",
    "            file_location = output_file_key\n",
    "        else:\n",
    "            # Fallback: save locally even in Lambda mode if AWS not available\n",
    "            fallback_filename = f'stock_data_{input_date}_{timestamp}.csv'\n",
    "            with open(fallback_filename, 'w', newline='', encoding='utf-8') as f:\n",
    "                f.write(output.getvalue())\n",
    "            print(f\"üìÅ AWS not available, data saved locally as: {fallback_filename}\")\n",
    "            file_location = fallback_filename\n",
    "        \n",
    "        # Generate presigned URL (only if AWS available)\n",
    "        presigned_url = None\n",
    "        if aws_available and s3:\n",
    "            try:\n",
    "                presigned_url = s3.generate_presigned_url(\n",
    "                    'get_object',\n",
    "                    Params={'Bucket': bucket_name, 'Key': output_file_key},\n",
    "                    ExpiresIn=604800  # 7 days\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Could not generate presigned URL: {e}\")\n",
    "        \n",
    "        # Calculate statistics\n",
    "        success_rate = (len(valid_data) / len(tickers)) * 100 if tickers else 0\n",
    "        avg_api_calls_per_ticker = api_call_count / len(tickers) if tickers else 0\n",
    "        \n",
    "        # Send notification (only if AWS available)\n",
    "        if aws_available and sns:\n",
    "            message = (\n",
    "                f'üìä ESSENTIAL STOCK DATA COLLECTION COMPLETE\\n\\n'\n",
    "                f'üéØ STATISTICS:\\n'\n",
    "                f'‚Ä¢ Environment: {\"TEST\" if is_test else \"PRODUCTION\"}\\n'\n",
    "                f'‚Ä¢ Total tickers processed: {len(tickers):,}\\n'\n",
    "                f'‚Ä¢ Successful records: {len(valid_data):,}\\n'\n",
    "                f'‚Ä¢ Failed records: {len(errors):,}\\n'\n",
    "                f'‚Ä¢ Success rate: {success_rate:.1f}%\\n'\n",
    "                f'‚Ä¢ Processing time: {processing_time:.1f} seconds\\n'\n",
    "                f'‚Ä¢ API calls made: {api_call_count:,} (2 per ticker)\\n'\n",
    "                f'‚Ä¢ Avg API calls per ticker: {avg_api_calls_per_ticker:.1f}\\n\\n'\n",
    "                f'üìà DATA INCLUDES:\\n'\n",
    "                f'‚Ä¢ Ticker symbol\\n'\n",
    "                f'‚Ä¢ Company name\\n'\n",
    "                f'‚Ä¢ Market cap (millions)\\n'\n",
    "                f'‚Ä¢ Primary exchange\\n'\n",
    "                f'‚Ä¢ OHLCV data (Open, High, Low, Close, Volume)\\n'\n",
    "                f'‚Ä¢ Current price\\n\\n'\n",
    "                f'üíæ Download link (expires in 7 days):\\n{presigned_url or \"N/A\"}'\n",
    "            )\n",
    "            \n",
    "            if errors and len(errors) <= 10:\n",
    "                message += f'\\n\\n‚ùå ERRORS:\\n' + '\\n'.join(errors[:10])\n",
    "            elif errors:\n",
    "                message += f'\\n\\n‚ùå First 10 errors:\\n' + '\\n'.join(errors[:10])\n",
    "            \n",
    "            try:\n",
    "                sns.publish(\n",
    "                    TopicArn=sns_topic_arn,\n",
    "                    Subject=f'üìä Essential Stock Data Complete - {len(valid_data):,} stocks',\n",
    "                    Message=message\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Could not send SNS notification: {e}\")\n",
    "        \n",
    "        # Print summary for Jupyter mode\n",
    "        if jupyter_mode:\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"üéâ DATA COLLECTION COMPLETE!\")\n",
    "            print(\"=\"*60)\n",
    "            print(f\"üìä Processed {len(tickers):,} tickers\")\n",
    "            print(f\"‚úÖ Successful: {len(valid_data):,} ({success_rate:.1f}%)\")\n",
    "            print(f\"‚ùå Errors: {len(errors):,}\")\n",
    "            print(f\"‚è±Ô∏è  Time: {processing_time:.1f}s\")\n",
    "            print(f\"üîå API calls: {api_call_count:,} (2 per ticker)\")\n",
    "            print(f\"üìÅ File: {file_location}\")\n",
    "            print(f\"üîÑ Mode: {'Appended to existing' if existing_data else 'Created new'} daily file\")\n",
    "            if errors:\n",
    "                print(f\"\\nüö® Sample errors: {errors[:3]}\")\n",
    "        \n",
    "        return {\n",
    "            'statusCode': 200,\n",
    "            'body': {\n",
    "                'message': 'Successfully collected essential stock data',\n",
    "                'environment': 'test' if is_test else 'production',\n",
    "                'mode': 'jupyter' if jupyter_mode else 'lambda',\n",
    "                'append_mode': len(existing_data) > 0,\n",
    "                'existing_tickers': len(existing_data),\n",
    "                'total_tickers': len(tickers),\n",
    "                'successful_records': len(valid_data),\n",
    "                'errors': len(errors),\n",
    "                'success_rate': f\"{success_rate:.1f}%\",\n",
    "                'processing_time_seconds': round(processing_time, 1),\n",
    "                'api_calls_made': api_call_count,\n",
    "                'output_file': file_location,\n",
    "                'download_url': presigned_url\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_message = f'Error collecting essential stock data: {str(e)}'\n",
    "        print(f\"Exception: {error_message}\")\n",
    "        \n",
    "        # Send error notification (only if AWS available)\n",
    "        if aws_available and sns:\n",
    "            try:\n",
    "                sns.publish(\n",
    "                    TopicArn=sns_topic_arn,\n",
    "                    Subject='‚ùå Essential Stock Data Collection Failed',\n",
    "                    Message=f'Error: {error_message}\\n\\nProcessed {processed_count} tickers before failure.\\nAPI calls made: {api_call_count}'\n",
    "                )\n",
    "            except Exception as sns_error:\n",
    "                print(f\"Could not send error notification: {sns_error}\")\n",
    "        \n",
    "        return {\n",
    "            'statusCode': 500,\n",
    "            'body': {'error': error_message, 'processed_count': processed_count, 'api_calls': api_call_count}\n",
    "        }\n",
    "result = lambda_handler()\n",
    "# For Lambda: gets called automatically with event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1e023f-9999-4cbe-849f-bca8963a3c2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
